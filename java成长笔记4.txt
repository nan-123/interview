java成长笔记4
14. Hbase
 base 是分布式、面向列的开源数据库（其实准确的说是面向列族）
 HDFS 为 Hbase 提供可靠的底层数据存储服务
 MapReduce 为 Hbase 提供高性能的计算能力
 Zookeeper 为 Hbase 提供稳定服务和 Failover 机制
14.1.2. 列式存储
 列方式所带来的重要好处之一就是，由于查询中的选择规则是通过列来定义的，因此整个数据库
 是自动索引化的
14.1.3. Hbase 核心概念
 14.1.3.1. Column Family 列族
  Column Family 又叫列族，Hbase 通过列族划分数据的存储，列族下面可以包含任意多的列，实
  现灵活的数据存取。Hbase 表的创建的时候就必须指定列族。就像关系型数据库创建的时候必须
  指定具体的列是一样的。Hbase 的列族不是越多越好，官方推荐的是列族最好小于或者等于 3。我
  们使用的场景一般是 1 个列族
 14.1.3.2. Rowkey（Rowkey 查询，Rowkey 范围扫描，全表扫描）
   Rowkey 的概念和 mysql 中的主键是完全一样的
 14.1.3.3. Region 分区
   Region 的概念和关系型数据库的分区或者分片差不多
   Hbase 会将一个大表的数据基于 Rowkey 的不同范围分配到不通的 Region 中
 14.1.3.4. TimeStamp 多版本
  在Hbase 中，相同 rowkey 的数据按照 timestamp 倒序排列。默认查询的是最新的版本，用户
  可同指定 timestamp 的值来读取旧版本的数据
14.1.4. Hbase 核心架构
  Hbase 是由 Client、Zookeeper、Master、HRegionServer、HDFS 等几个组建组成。
   14.1.4.1. Client：
    Client 包含了访问 Hbase 的接口，另外 Client 还维护了对应的 cache 来加速 Hbase 的
    访问，比如 cache 的.META.元数据的信息。
   14.1.4.2. Zookeeper：
    Hbase 通过 Zookeeper 来做 master 的高可用、RegionServer 的监控、元数据的入口
    以及集群配置的维护等工作
   14.1.4.3. Hmaster
    1. 为 RegionServer 分配 Region
    2. 维护整个集群的负载均衡
    3. 维护集群的元数据信息发现失效的 Region，并将失效的 Region 分配到正常
       RegionServer 上当 RegionSever 失效的时候，协调对应 Hlog 的拆分
   14.1.4.4. HregionServer
     HregionServer 直接对接用户的读写请求，是真正的“干活”的节点
	  1. 管理 master 为其分配的 Region
	  2. 处理来自客户端的读写请求
      3. 负责和底层 HDFS 的交互，存储数据到 HDFS
      4. 负责 Region 变大以后的拆分
      5. 负责 Storefile 的合并工作
   14.1.4.5. Region 寻址方式（通过 zookeeper .META）
   14.1.4.6. HDFS
    HDFS 为 Hbase 提供最终的底层数据存储服务，同时为 Hbase 提供高可用（Hlog 存储在
    HDFS）的支持。
   14.1.5.2. MemStore 刷盘
    为了提高 Hbase 的写入性能，当写请求写入 MemStore 后，不会立即刷盘。而是会等到一定的时候进行刷盘的操作
  1：hbase.regionserver.global.memstore.upperLimit，默认为整个 heap 内存的 40%。
  2. 当 MemStore 的大小达到 hbase.hregion.memstore.flush.size 大小的时候会触发刷
     盘，默认 128M 大小
  3：当达到 Hlog 的最大个数
     的时候，会强制刷盘。这个参数是 hase.regionserver.max.logs，默认是 32 个。
  4. 可以通过 hbase shell 或者 java api 手工触发 flush 的操作。
  5. 在正常关闭 RegionServer 会触发刷盘的操作
  6：Region 使用 HLOG 恢复完数据后触发
总结：大文件，强一致性，m-s架构，读写性能低，乐观并发控制

15. MongoDB
 MongoDB 是由 C++语言编写的，是一个基于分布式文件存储的开源数据库系统
  在高负载的情况下，添加更多的节点，可以保证服务器性能
  15.1.2. 特点
    x MongoDB 是一个面向文档存储的数据库，操作起来比较简单和容易。
    x 你可以在 MongoDB 记录中设置任何属性的索引 (如：FirstName="Sameer",Address="8 Ga
    ndhi Road")来实现更快的排序。
    x 你可以通过本地或者网络创建数据镜像，这使得 MongoDB 有更强的扩展性。
    x 如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其
    他节点上这就是所谓的分片。 x Mongo 支持丰富的查询表达式。查询指令使用 JSON 形式的标记，可轻易查询文档中内嵌的
    对象及数组。
    x MongoDb 使用 update()命令可以实现替换完成的文档（数据）或者一些指定的数据字段 。 x Mongodb 中的 Map/reduce 主要是用来对数据进行批量处理和聚合操作。
    x Map 和 Reduce。Map 函数调用 emit(key,value)遍历集合中所有的记录，将 key 与 value 传 给 Reduce 函数进行处理。
    x Map 函数和 Reduce 函数是使用 Javascript 编写的，并可以通过 db.runCommand 或 mapre
    duce 命令来执行 MapReduce 操作
    xGridFS 是 MongoDB 中的一个内置功能，可以用于存放大量小文件。 x MongoDB 允许在服务端执行脚本，可以用 Javascript 编写某个函数，直接在服务端执行，也
    x可以把函数的定义存储在服务端，下次直接调用即可。
16. Cassandra
 Apache Cassandra 是高度可扩展的，高性能的分布式 NoSQL 数据库
 16.1.2. 数据模型
  Key Space（对应 SQL 数据库中的 database）
  Key（对应 SQL 数据库中的主键）
  column（对应 SQL 数据库中的列）
  super column（SQL 数据库不支持）cassandra 允许 key/value 中的 value 是一个 map(key/value_list)，即某个 column 有多个子列。
  Standard Column Family（相对应 SQL 数据库中的 table）
  Super Column Family（SQL 数据库不支持）
 16.1.3. Cassandra 一致 Hash 和虚拟节点
  一致性 Hash（多米诺 down 机）
  为每个节点分配一个 token，根据这个 token 值来决定节点在集群中的位置以及这个节点所存储的数据范围。
 16.1.4. Gossip 协议 “闲话算法”
------------------------------------------------------------------------------------------------------
17. 设计模式
 17.1.2. 工厂方法模式
17.1.3. 抽象工厂模式
17.1.4. 单例模式
17.1.5. 建造者模式
17.1.6. 原型模式
17.1.7. 适配器模式
17.1.8. 装饰器模式
17.1.9. 代理模式
17.1.10. 外观模式
17.1.11. 桥接模式
17.1.12. 组合模式
17.1.13. 享元模式
17.1.14. 策略模式
17.1.15. 模板方法模式
17.1.16. 观察者模式
17.1.17. 迭代子模式
17.1.18. 责任链模式
17.1.19. 命令模式
17.1.20. 备忘录模式
17.1.21. 状态模式
17.1.22. 访问者模式
17.1.23. 中介者模式
17.1.24. 解释器模式
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
18. 负载均衡
 负载均衡 建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带
  宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性
 18.1.1.1. 四层负载均衡（目标地址和端口交换）
    主要通过报文中的目标地址和端口，再加上负载均衡设备设置的服务器选择方式，决定最终选择
     的内部服务器
	以常见的 TCP 为例，【负载均衡设备在接收到第一个来自客户端的 SYN 请求时】，即通过上述方式选
     择一个【最佳的服务器】，并对【报文中目标 IP 地址进行修改(改为后端服务器 IP）】，直接转发给该服务
     器 负载均衡设备只是起到一个类似路由器的转发动作
   实现四层负载均衡的软件有：
    F5：硬件负载均衡器，功能很好，但是成本很高。
    lvs：重量级的四层负载软件。
    nginx：轻量级的四层负载软件，带缓存功能，正则表达式较灵活。
    haproxy：模拟四层转发，较灵活。
 18.1.1.2. 七层负载均衡（内容交换）
  概念：
   所谓七层负载均衡，也称为“内容交换”，也就是主要通过报文中的真正有意义的应用层内容，
   再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器
  七层应用负载的好处，是使得整个网络更智能化
   例如访问一个网站的用户流量，可以通过七层
   的方式，将对图片类的请求转发到特定的图片服务器并可以使用缓存技术；将对文字类的请求可
   以转发到特定的文字服务器并可以使用压缩技术
 实现七层负载均衡的软件有
  haproxy：天生负载均衡技能，全面支持七层代理，会话保持，标记，路径转移；
  nginx：只在 http 协议和 mail 协议上功能比较好，性能与 haproxy 差不多；
  apache：功能较差
  Mysql proxy：功能尚可。
 18.1.2. 负载均衡算法/策略
  18.1.2.1. 轮循均衡（Round Robin）- 服务器配置相同
  18.1.2.1. 轮循均衡（Round Robin）- 服务器配置不一样，好的权重高
  18.1.2.3. 随机均衡（Random）
  18.1.2.4. 权重随机均衡（Weighted Random）
  18.1.2.5. 响应速度均衡（Response Time 探测时间）
   负载均衡设备对内部各服务器发出一个探测请求（例如 Ping）
   然后根据内部中各服务器对探测请求的最快响应时间来决定哪一台服务器来响应客户端的服务请求
   仅仅指的是负载均衡设备与服务器间的
  18.1.2.6. 最少连接数均衡（Least Connection）
  18.1.2.7. 处理能力均衡（CPU、内存），尤其适合运用到第七层（应用层）
  18.1.2.8. DNS 响应均衡（Flash DNS）地域最快
  18.1.2.9. 哈希算法-相同的请求发到相同的服务器
  18.1.2.10. IP 地址散列（保证客户端服务器对应关系稳定）相同客户端服务器相同
  18.1.2.11.URL 散列-相同url服务器相同
 18.1.3.1. LVS 原理
  LVS 的 IP 负载均衡技术是通过 IPVS 模块来实现的，IPVS 是 LVS 集群系统的核心软件
  安装在 Director Server 上，同时在 Director Server 上虚拟出一个 IP 地址
  用户必须通过这个虚拟的 IP 地址访问服务器。这个虚拟 IP 一般称为 LVS 的 VIP，即 Virtual IP
  访问的请求首先经过 VIP 到达负载调度器
  然后由负载调度器从 Real Server 列表中选取一个服务节点响应用户的请求
  如何发请求到真实服务器，怎么返回，是 IPVS 实现的重点技术。
  ipvs ： 工作于内核空间，主要用于使用户定义的策略生效
  ipvsadm : 工作于用户空间，主要用于用户定义和管理集群服务的工具
 18.1.3.1. LVS NAT 模式
  ①.客户端将请求发往前端的负载均衡器，请求报文源地址是 CIP(客户端 IP),后面统称为 CIP)，目
    标地址为 VIP(负载均衡器前端地址，后面统称为 VIP)。
  ②.负载均衡器收到报文后，发现请求的是在规则里面存在的地址，那么它将客户端请求报文的目
    标地址改为了后端服务器的 RIP 地址并将报文根据算法发送出去。
  ③.报文送到 Real Server 后，由于报文的目标地址是自己，所以会响应该请求，并将响应报文返还
    给 LVS。
  ④.然后 lvs 将此报文的源地址修改为本机并发送给客户端。
  注意：在 NAT 模式中，Real Server 的网关必须指向 LVS，否则报文无法送达客户端
  特点：
   1、NAT 技术将请求的报文和响应的报文都需要通过 LB 进行地址改写，因此网站访问量比较大的
   时候 LB 负载均衡调度器有比较大的瓶颈，一般要求最多之能 10-20 台节点
   2、只需要在 LB 上配置一个公网 IP 地址就可以了。 3、每台内部的 realserver 服务器的网关地址必须是调度器 LB 的内网地址。
   4、NAT 模式支持对 IP 地址和端口进行转换。即用户请求的端口和真实服务器的端口可以不一致。
  优点：
   集群中的物理服务器可以使用任何支持 TCP/IP 操作系统，只有负载均衡器需要一个合法的 IP 地
   址。
  缺点：
   扩展性有限。当服务器节点（普通 PC 服务器）增长过多时,【负载均衡器】将成为整个系统的瓶颈
 18.1.3.2. LVS DR 模式（局域网改写 mac 地址）
   ①.客户端将请求发往前端的负载均衡器，请求报文源地址是 CIP，目标地址为 VIP。
   ②.负载均衡器收到报文后，发现请求的是在规则里面存在的地址，那么它将客户端请求报文的源
     MAC 地址改为自己 DIP 的 MAC 地址，目标 MAC 改为了 RIP 的 MAC 地址，并将此包发送给 RS。
   ③.RS 发现请求报文中的目的 MAC 是自己，就会将次报文接收下来，处理完请求报文后，将响应
     报文通过 lo 接口送给 eth0 网卡直接发送给客户端。
    注意：需要设置 lo 接口的 VIP 不能响应本地网络内的 arp 请求。
  总结：
   1、通过在调度器 LB 上修改数据包的目的 MAC 地址实现转发。注意源地址仍然是 CIP，目的地址
   仍然是 VIP 地址。 2、请求的报文经过调度器，而 RS 响应处理后的报文无需经过调度器 LB，因此并发访问量大时使
   用效率很高（和 NAT 模式比）
   3、因为 DR 模式是通过 MAC 地址改写机制实现转发，因此所有 RS 节点和调度器 LB 只能在一个
   局域网里面  
  优点：
   和 TUN（隧道模式）一样，负载均衡器也只是分发请求，应答包通过单独的路由方法返回给客户
   端。与 VS-TUN 相比，VS-DR 这种实现方式不需要隧道结构，因此可以使用大多数操作系统做为
   物理服务器。
   DR 模式的效率很高，但是配置稍微复杂一点，因此对于访问量不是特别大的公司可以用
   haproxy/nginx取代。日1000-2000W PV或者并发请求1万一下都可以考虑用haproxy/nginx。
   缺点：
   所有 RS 节点和调度器 LB 只能在一个局域网里面
 18.1.3.3. LVS TUN 模式（IP 封装、跨网段）
  ①.客户端将请求发往前端的负载均衡器，请求报文源地址是 CIP，目标地址为 VIP。
  ②.负载均衡器收到报文后，发现请求的是在规则里面存在的地址，那么它将在客户端请求报文的
  首部再封装一层 IP 报文,将源地址改为 DIP，目标地址改为 RIP,并将此包发送给 RS。 
  ③.RS 收到请求报文后，会首先拆开第一层封装,然后发现里面还有一层 IP 首部的目标地址是自己
  lo 接口上的 VIP，所以会处理次请求报文，并将响应报文通过 lo 接口送给 eth0 网卡直接发送给客
  户端。
  注意：需要设置 lo 接口的 VIP 不能在共网上出现。
  总结：
  1.TUNNEL 模式必须在所有的 realserver 机器上面绑定 VIP 的 IP 地址
  2.TUNNEL 模式的 vip ------>realserver 的包通信通过 TUNNEL 模式，不管是内网和外网都能通
  信，所以不需要 lvs vip 跟 realserver 在同一个网段内。
  3.TUNNEL 模式 realserver 会把 packet 直接发给 client 不会给 lvs 了
  4.TUNNEL 模式走的隧道模式，所以运维起来比较难，所以一般不用。
 优点：
  负载均衡器只负责将请求包分发给后端节点服务器，而 RS 将应答包直接发给用户。所以，减少了
  负载均衡器的大量数据流动，负载均衡器不再是系统的瓶颈，就能处理很巨大的请求量，这种方
  式，一台负载均衡器能够为很多 RS 进行分发。而且跑在公网上就能进行不同地域的分发。
  缺点：
  隧道模式的 RS 节点需要合法 IP，这种方式需要所有的服务器支持”IP Tunneling”(IP 
  Encapsulation)协议，服务器可能只局限在部分 Linux 系统上
 18.1.3.4. LVS FULLNAT 模式
  无论是 DR 还是 NAT 模式，不可避免的都有一个问题：LVS 和 RS 必须在同一个 VLAN 下，否则
  LVS 无法作为 RS 的网关。这引发的两个问题是：
  1、同一个 VLAN 的限制导致运维不方便，跨 VLAN 的 RS 无法接入。
  2、LVS 的水平扩展受到制约。当 RS 水平扩容时，总有一天其上的单点 LVS 会成为瓶颈。
  Full-NAT 由此而生，解决的是 LVS 和 RS 跨 VLAN 的问题，而跨 VLAN 问题解决后，LVS 和 RS 
  不再存在 VLAN 上的从属关系，可以做到多个 LVS 对应多个 RS，解决水平扩容的问题。
  Full-NAT 相比 NAT 的主要改进是，在 SNAT/DNAT 的基础上，加上另一种转换，转换过程如下：
  1. 在包从 LVS 转到 RS 的过程中，源地址从客户端 IP 被替换成了 LVS 的内网 IP。内网 IP 之间
  可以通过多个交换机跨 VLAN 通信。目标地址从 VIP 修改为 RS IP.
  2. 当 RS 处理完接受到的包，处理完成后返回时，将目标地址修改为 LVS ip，原地址修改为 RS 
  IP，最终将这个包返回给 LVS 的内网 IP，这一步也不受限于 VLAN。
  3. LVS 收到包后，在 NAT 模式修改源地址的基础上，再把 RS 发来的包中的目标地址从 LVS 内 网 IP 改为客户端的 IP,并将原地址修改为 VIP。
  Full-NAT 主要的思想是把网关和其下机器的通信，改为了普通的网络通信，从而解决了跨 VLAN 
  的问题。采用这种方式，LVS 和 RS 的部署在 VLAN 上将不再有任何限制，大大提高了运维部署的
  便利性
 总结
 1. FULL NAT 模式不需要 LBIP 和 realserver ip 在同一个网段；
 2. full nat 因为要更新 sorce ip 所以性能正常比 nat 模式下降 10%
 
18.1.4. Keepalive
 keepalive 起初是为 LVS 设计的，专门用来监控 lvs 各个服务节点的状态，后来加入了 vrrp 的功
 能，因此除了 lvs，也可以作为其他服务（nginx，haproxy）的高可用软件
18.1.5. Nginx 反向代理负载均衡
 普通的负载均衡软件，如 LVS，其实现的功能只是对请求数据包的转发、传递，从负载均衡下的节
 点服务器来看，接收到的请求还是来自访问负载均衡器的客户端的真实用户；而反向代理就不一
 样了，【反向代理服务器在接收访问用户请求后，会代理用户 重新发起请求代理下的节点服务器】，
 最后把数据返回给客户端用户。在节点服务器看来，访问的节点服务器的客户端用户就是反向代
 理服务器，而非真实的网站访问用户。
 18.1.5.1. upstream_module 和健康检测
  ngx_http_upstream_module 是负载均衡模块，可以实现网站的负载均衡功能即节点的健康检
  查 upstream 模块允许 Nginx 定义一组或多组节点服务器组，使用时可通过 proxy_pass 代理方
  式把网站的请求发送到事先定义好的对应 Upstream 组 的名字上
  upstream lvsServer{
 server 191.168.1.11 weight=5 ;
 server 191.168.1.22:82;
 server example.com:8080 max_fails=2 fail_timeout=10s backup;
 #域名的话需要解析的哦，内网记得 hosts
}
 18.1.5.1. proxy_pass 请求转发
  proxy_pass 指令属于 ngx_http_proxy_module 模块
  此模块可以将请求转发到另一台服务器，
  在实际的反向代理工作中，会通过 location 功能匹配指定的 URI，然后把接收到服务匹配 URI 的
  请求通过 proyx_pass 抛给定义好的 upstream 节点池。
  location /download/ {
 proxy_pass http://download/vedio/;
}
  proxy 模块参数               说明
 proxy_next_upstream 什么情况下将请求传递到下一个 upstream
 proxy_limite_rate 限制从后端服务器读取响应的速率
 proyx_set_header 设置 http 请求 header 传给后端服务器节点，如：可实现让代
 理后端的服务器节点获取访问客户端的这是 ip
 client_body_buffer_size 客户端请求主体缓冲区大小
 proxy_connect_timeout 代理与后端节点服务器连接的超时时间
 proxy_send_timeout 后端节点数据回传的超时时间
 proxy_read_timeout 设置 Nginx 从代理的后端服务器获取信息的时间，表示连接成
 功建立后，Nginx 等待后端服务器的响应时间
 proxy_buffer_size 设置缓冲区大小
 proxy_buffers 设置缓冲区的数量和大小
 proyx_busy_buffers_size 用于设置系统很忙时可以使用的 proxy_buffers 大小，推荐为
 proxy_buffers*2
 proxy_temp_file_write_size 指定 proxy 缓存临时文件的大小